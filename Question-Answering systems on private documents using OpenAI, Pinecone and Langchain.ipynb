{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9642dda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175435ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d90bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8b5427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88af19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    \n",
    "    import os\n",
    "    name, extensions = os.path.splitext(file)\n",
    "    \n",
    "    if extensions == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading{file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading{file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print(\"Document not valid, please introduce only PDF'S or Docx formats\")\n",
    "        return None\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b717947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data,chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c786279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(text):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in text])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens/ 1000*0.004:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc2d8d",
   "metadata": {},
   "source": [
    "### Embedding and Uploading to a Vector Database(Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5be06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name):\n",
    "    import pinecone\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    \n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b93cd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):#the pinecone free only supports 1 index so migth be necessery to delete it frecuently\n",
    "    import pinecone\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    \n",
    "    if index_name == 'all':\n",
    "        indexes = pinecone.list_indexes()\n",
    "        print(f'Deleting all indexes...')\n",
    "        for index in indexes:\n",
    "            pinecone.delete_index(index)\n",
    "        print('ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ...',end='')\n",
    "        pinecnone.delete_index(index_name)\n",
    "        print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a96b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "    \n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.run(q)\n",
    "    return answer\n",
    "\n",
    "def ask_with_memory(vector_store, question, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    \n",
    "    crc = ConversationalRetrievalChain.from_llm(llm, retriever)\n",
    "    result = crc({'question': question, 'chat_history': chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    \n",
    "    return result, chat_history\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2421b",
   "metadata": {},
   "source": [
    "### Running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51d6657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadingdocument.pdf\n",
      ".ࠂReinforcement Learning\n",
      "Introduction\n",
      "Imagine you are lost in a labyrinth and have to ˪nd your way out. As you are there for\n",
      "the ˪rst time, you do not know which way to choose to reach the door to leave. More-\n",
      "over, there are dangerous ˪elds  on the labyrinth and you should avoid stepping on\n",
      "them.\n",
      "You will have four actions you can perform: move up, down, left, or right. As you do not\n",
      "know the labyrinth, the only way to ˪nd your way out is to see what happens when you\n",
      "perform random actions. Within the learning process, you will ˪nd out that there are\n",
      "˪elds  on the labyrinth that will reward you by letting you escape the labyrinth. How-\n",
      "ever, there are also ˪elds  where you will receive a negative reward as they are danger-\n",
      "ous to step on. After some time, you will manage to ˪nd your way out without stepping\n",
      "on the dangerous ˪elds  from the experience you have made walking around. This proc-\n",
      "ess of learning by reward is called reinforcement learning.\n",
      "In this unit, you will learn more about the basic ideas of reinforcement learning and\n",
      "the underlying principles. Moreover, you will get to know algorithms, such as Q-learn-\n",
      "ing, that can help you optimize the learning experience.\n",
      "ࠀ.ࠂࠀ.ࠂWhat is Reinforcement Learning?\n",
      "Generally, in machine learning, there exist three techniques to train a speci˪c  learning\n",
      "model: supervised, unsupervised, and reinforcement learning.߿ࠃ ࠂ\n",
      "{'source': 'document.pdf', 'page': 3}\n",
      "you have 8 in your data\n"
     ]
    }
   ],
   "source": [
    "data = load_document('document.pdf')\n",
    "print(data[1].page_content)\n",
    "print(data[3].metadata)\n",
    "print(f'you have {len(data)} in your data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2136cd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was initially released on March 14, 2023, and has been made publicly available via the paid chatbot product ChatGPT Plus, and via OpenAI's API.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4 is also capable of taking images as input, though this feature has not been made available since launch. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "\n",
      "\n",
      "== Background ==\n",
      " \n",
      "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\n",
      "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
      "\n",
      "\n",
      "== Capabilities ==\n",
      "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams.To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his programs from MATLAB to Python went down from days to \"an hou\n"
     ]
    }
   ],
   "source": [
    "data_2 = load_from_wikipedia('GPT-4')\n",
    "print(data_2[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2c8c6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "# print(chunks[11].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb027212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 3628\n",
      "Embedding Cost in USD: 0.014512\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d0fac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes...\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abe8bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadocument and embeddings ...ok\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1eb9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document introduces the principles of reinforcement learning and explains Markov decision processes. It also covers the Q-learning algorithm. Completing this unit will enable understanding and application of these concepts in the field of reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "q = 'Make a 3 lines resume about the document'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d0135db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit\n",
      "Question #1 Make a summary of this document\n",
      "\n",
      "Answer: I apologize, but I don't have access to the document you're referring to. Could you please provide the document's content or give me more information about it?\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      "Question #2 quit\n",
      "Quitting....bye bye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit')\n",
    "\n",
    "while True:\n",
    "    q=input(f'Question #{i} ')\n",
    "    i = i+1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting....bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}') #This and the line after is just for readibility\n",
    "    print(f'\\n {\"-\" * 50}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f7fdda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes...\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5348bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index chatgpt already exists. Loading embeddings ... Ok\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('ChatGPT', 'es')\n",
    "chunks = chunk_data(data)\n",
    "index_name = 'chatgpt'\n",
    "vector_store = insert_or_fetch_embeddings(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6ef4dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat GPT-3 es un modelo de lenguaje de inteligencia artificial desarrollado por OpenAI. GPT-3 significa \"Generative Pre-trained Transformer 3\", y se refiere a la tercera generación de modelos GPT. Es conocido por su capacidad para generar texto coherente y natural en respuesta a preguntas o instrucciones dadas por los usuarios. GPT-3 ha demostrado ser muy poderoso y versátil en una amplia gama de aplicaciones, desde asistencia en la escritura hasta generación de código.\n"
     ]
    }
   ],
   "source": [
    "q = 'Que es chat-gpt3'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b77f8e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Markov property is a key characteristic of Markov processes. It states that the future state of a process is independent of its past history, given its present state. In other words, the probability distribution of future states depends only on the current state and not on how the system arrived at that state.\n",
      "[('Wat is the markov property?', 'The Markov property is a key characteristic of Markov processes. It states that the future state of a process is independent of its past history, given its present state. In other words, the probability distribution of future states depends only on the current state and not on how the system arrived at that state.')]\n"
     ]
    }
   ],
   "source": [
    "# asking with memory\n",
    "chat_history = []\n",
    "question = 'Wat is the markov property?'\n",
    "result, chat_history = ask_with_memory(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fad70d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Markov property states that the future state of a system depends only on its current state, and not on the past states or how the system arrived at that state.\n",
      "[('Wat is the markov property?', 'The Markov property is a key characteristic of Markov processes. It states that the future state of a process is independent of its past history, given its present state. In other words, the probability distribution of future states depends only on the current state and not on how the system arrived at that state.'), ('Make it shorter', 'The Markov property states that the future state of a system depends only on its current state, and not on the past states or how the system arrived at that state.')]\n"
     ]
    }
   ],
   "source": [
    "question = 'Make it shorter'\n",
    "result, chat_history = ask_with_memory(vector_store, question, chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdacd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
